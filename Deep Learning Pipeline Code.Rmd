---
title: "Deep Learning with R"
author: "Chenshu Liu"
date: "April 2022"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
  html_document: default
---

\newpage
\section{Dense Layer Model}
\subsection{Data \& Library}
```{r}
# import spreadsheet files
library(readr)
# deep learning package
library(keras)

setwd("~/Documents/Programming/R/Deep Learning with R/Datasets")
data <- read_csv("SimulatedBinaryClassificationDataset.csv",
                 col_names = TRUE)
```
\subsection{Data Preprocessing}
```{r}
# data.frame --> matrix
data  <- as.matrix(data)
# remove the row and col names, leaving only numerical values
dimnames(data) = NULL
mode(data)
```
\subsection{Train Test Split}
```{r}
# train and test split index
set.seed(123)
index <- sample(2,
                nrow(data),
                replace = TRUE,
                prob = c(0.9, 0.1))

# data splitting
x_train <- data[index == 1, 1:10]
x_test <- data[index == 2, 1:10]
y_test_actual <- data[index == 2, 11]

# use teh to_categorical function in keras package for one-hot encoding
y_train <- to_categorical(data[index == 1, 11])
y_test <- to_categorical(data[index == 2, 11])
```
\subsection{Modeling}
```{r}
model <- keras_model_sequential() %>%
  # layer_dense means a densely connected layer
  layer_dense(name = "DeepLayer1",
              units = 10, # hyperparameter: the number of nodes
              activation = "relu",
              # the first layer need to have specification about the input dimension
              input_shape = c(10)) %>%
  layer_dense(name = "DeepLayer2",
              units = 10,
              activation = "relu") %>%
  layer_dense(name = "OutputLayer",
              units = 2,
              # softmax function will provide probabilities of the nodes
              activation = "softmax")

summary(model)

model %>% compile(
  # another way to calculate loss, besides mean-squared-error
  loss = "categorical_crossentropy", 
  # a special way of gradient descent
  optimizer = "adam",
  # measurement of model performance - using accuracy to measure
  metrics = c("accuracy")) 

history <- model %>%
  fit(x_train,
      y_train,
      # number of full forward & backward propagation
      # (i.e. run 10 times back and forth of all samples)
      epoch = 10, 
      # instead of propagating the whole dataset at one go, use smaller batches
      batch_size = 256, 
      # splitting the training set to test itself during training
      validation_split = 0.1, 
      verbose = 2)

# plot the training history
plot(history)

model %>%
  evaluate(x_test,
           # NOTE: here we are still using the one-hot encoded y_test
           y_test)

# form predictions
pred <- model %>%
  predict(x_test) %>%
  k_argmax()
# reference for converting tensor to R data types
# https://torch.mlverse.org/technical/tensors/
pred <- as.array(pred)
table(Predicted = pred,
      # NOTE: for confusion matrix, we are using the original y_test_actual, not encoded
      Actual = y_test_actual)
```

\newpage
\section{Regularization}
\subsection{Data \& Library}
```{r}
# Applying regularization to deal with overfitting
library(keras)
library(readr)
library(tidyr)
library(tibble)
library(plotly)

# specify the number of feature variables for the dataset to be downloaded
num_words <- 5000
imdb <- dataset_imdb(num_words = num_words)

# train test split
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
```
\subsection{Data Preprocessing}
```{r}
# multi-hot encoding
multi_hot_sequences <- function(sequences, dimension){
  multi_hot <- matrix(0, 
                      # the number of samples in the sequences
                      # sequences are stored as lists
                      nrow = length(sequences), 
                      ncol = dimension)
  for(i in 1 : length(sequences)){
    # sequences[[i]] extracts the label of the words in the text sample i
    # which ever word is included in that sequence will be assigned 1 at row i
    multi_hot[i, sequences[[i]]] <- 1
  }
  multi_hot
}

train_data <- multi_hot_sequences(train_data, num_words)
test_data <- multi_hot_sequences(test_data, num_words)
```
\subsection{L2 Regularization Model}
```{r}
l2_model <-
  keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = num_words,
              # apply regularization in the layer_dense function's argument
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 16, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")

l2_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = list("accuracy")
)

l2_model %>% summary()

l2_history <- l2_model %>% fit(
  train_data,
  train_labels,
  epoch = 20,
  batch_size = 512,
  validation_data = list(test_data, test_labels),
  verbose = 2
)

plot(l2_history)
```
\subsection{Dropout Regularization Model}
```{r}
drop_model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = num_words) %>%
  # a new layer to specify the dropout rate
  layer_dropout(0.6) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.6) %>%
  layer_dense(units = 1, activation = "sigmoid")

drop_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = list("accuracy")
)

drop_model %>% summary()

drop_history <- drop_model %>% fit(
  train_data,
  train_labels,
  epoch = 20,
  batch_size = 512,
  validation_data = list(test_data, test_labels),
  verbose = 2
)

plot(drop_history)
```

\newpage
\section{Neural Network Optimizations}
\subsection{Data \& Library}
```{r}
library(readr)
library(keras)

setwd("~/Documents/Programming/R/Deep Learning with R/Datasets")
data.set <- read_csv("RegressionData.csv",
                     col_names = FALSE)
```
\subsection{Data Preprocessing}
\subsubsection{General Preprocessing}
```{r}
# transform dataframe to matrix
data.set <- as.matrix(data.set)
# remove column names
dimnames(data.set) <- NULL

# train test split
set.seed(123)
index <- sample(2,
                nrow(data.set),
                replace = TRUE,
                prob = c(0.8, 0.2))

x_train <- data.set[index == 1, 1:10]
x_test <- data.set[index == 2, 1:10]
y_train <- data.set[index == 1, 11]
y_test <- data.set[index == 2, 11]
```
\subsubsection{Normalization}
```{r}
# normalizing data
mean.train <- apply(x_train, 2, mean)
sd.train <- apply(x_train, 2, sd)
x_train <- scale(x_train)
# use the normalizing parameters from training set to normalize testing set
x_test <- scale(x_test,
                center = mean.train,
                scale = sd.train)
```
\subsection{Modeling}
```{r}
# creating the model
model <- keras_model_sequential() %>%
  layer_dense(units = 25,
              activation = "relu",
              input_shape = c(10)) %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 25,
              activation = "relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 25,
              activation = "relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 1)

model %>% summary()

# compile the model
model %>% compile(
  # the metric for propagation
  loss = "mse",
  optimizer = optimizer_rmsprop(),
  # not for propagation, but for user feedback
  # letting us know the model's performance
  metrics = c("mean_absolute_error"))

# fitting the data
model_history <- model %>%
  fit(x_train,
      y_train,
      epoch = 50,
      batch_size = 32,
      validation_split = 0.1,
      callbacks = c(callback_early_stopping(monitor = "val_mean_absolute_error",
                                            patience = 5)),
      verbose = 2)

plot(model_history)

# testing the model
c(loss, mae) %<-% (model %>% evaluate(x_test, y_test, verbose = 0))
paste0("Mean Absolute Error on test set is:", mae)
```


\newpage
\section{Convolution Neural Network}
\subsection{Data \& Library}
```{r}
library(keras)

# a dataset of numerous images of hand-written numbers from 0-9
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```
\subsection{Data Preprocessing}
```{r}
img_row <- dim(x_train)[2]
img_col <- dim(x_test)[3]

# images should have three channels, but the data only has two, need transformation
# because this is gray-scale image, the third channel only has one layer
x_train <- array_reshape(x_train,
                         c(nrow(x_train),
                           img_row,
                           img_col, 1))
x_test <- array_reshape(x_test,
                        c(nrow(x_test),
                          img_row,
                          img_col, 1))
input_shape <- c(img_row, img_col, 1)

# normalize the datasets by dividing the number 255
# because the color gradient is from 0(black) to 255(white)
# dividing by 255 can transform the entries to values between 0 and 1
x_train <- x_train/255
x_test <- x_test/255

# use one-hot encoding to encode the y values
# y values are labels for number 0 to 9, thus we need 10 categories
y_train <- to_categorical(y_train, num_classes = 10)
y_test <- to_categorical(y_test, num_classes = 10)
```
\subsection{Modeling}
```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(
    # number of filters for transformation
    filters = 16,
    # size of the filters
    kernel_size = c(3,3),
    activation = 'relu',
    input_shape = input_shape) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 10,
              activation = 'relu') %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 10,
              # for categorical prediction
              activation = 'softmax')

model %>% summary()

model %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)

model %>% fit(
  x_train, 
  y_train,
  batch_size = 128,
  epochs = 12,
  validation_split = 0.2
)

score <- model %>% evaluate(x_test,
                            y_test)
score
```

